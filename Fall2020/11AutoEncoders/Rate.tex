\input ../../SlidePreamble
\input ../../preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \centerline{Rate-Distortion Autoencoders (RDAs)}
  \vfill
  \vfill

\slide{Cross Entropy for Continuous Structured $y$}

Cross entropy is a challenging objective for continuous structured values $y$ such as images and sounds.

\vfill
\begin{eqnarray*}
\Phi^* & = & \argmin_\Phi \;E_{y \sim \popd}\;-\ln p_\Phi(y) \\
\\
\Phi^* & = & \argmin_\Phi \;E_{\tuple{x,y} \sim \popd}\;-\ln p_\Phi(y|x)
\end{eqnarray*}

\vfill
GANs replace the cross-entropy loss with an adversarial discrimination loss.

\vfill
Rate-Distortion Auto-Encoders (RDAs) and Variational Auto-Encoders (VAEs) use the cross-entropy objective more directly.

\slide{Rate-Distortion Autoencoders (RDAs)}

A rate-distortion autoencoder (RDA) replaces differential cross-entropy loss with a compression rate and
a reconstruction loss (distortion).

\vfill
The primary example is lossy compression of images and audio.

\vfill
We take the compressed object to be discrete --- a file with a well defined length in bits.

\slide{Rate-Distortion Autoencoders (RDAs)}

We compress a continuous signal $y$ to a bit string (or other discrete object) $\tilde{z}_\Phi(y)$.

\vfill
We decompress $\tilde{z}_\Phi(y)$ to $y_\Phi(\tilde{z}_\Phi(y))$.

\vfill
We can then define a rate-distortion loss.

{\color{red} $${\cal L}(\Phi) = E_{y \sim \mathrm{Pop}}\;-\ln P_\Phi(\tilde{z}_\Phi(y)) + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))$$}

\vfill
Here the rate is defined as a discrete cross-entropy.

\slide{$L_2$ Distortion}

$${\cal L}(\Phi) = E_{y \sim \mathrm{Pop}}\;-\ln P_\Phi(\tilde{z}_\Phi(y)) + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))$$

\vfill
It is common to take

\begin{eqnarray*}
\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||^2 \\
\\
& = & -\ln p(y|\hat{y}) + C \;\;\;\;\;\mbox{for a Gaussian density}
\end{eqnarray*}

\vfill
We will ignore the log density interpretation and just call this distortion.

\slide{$L_1$ Distortion}

$${\cal L}(\Phi) = E_{y \sim \mathrm{Pop}}\;-\ln P_\Phi(\tilde{z}_\Phi(y)) + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))$$

Alternatively we have

\begin{eqnarray*}
\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||_1 \hspace{4em}(L_1) \\
\\
& = & -\ln p(y|\hat{y}) + C \;\;\mbox{for a Laplace density}
\end{eqnarray*}

\vfill
Again, we will ignore the log probability interpretation and just call this distortion.
\slide{CNN-based Image Compression}

These slides are loosely based on

\vfill
End-to-End Optimized Image Compression, Balle, Laparra, Simoncelli, ICLR 2017.


\vfill
\centerline{$y$\includegraphics[width=4in]{\images/deconvleft} $\;\tilde{z}\;$ \includegraphics[width=4in]{\images/deconvright}$\hat{y}$}


\slide{Rounding a Tensor}

Take $z_\Phi(y)$ can be a layer in a CNN applied to image $y$.  $z_\Phi(y)$ can have with both spatial and feature dimensions.

\vfill
Take $\tilde{z}_\Phi(y)$ to be the result of rounding each component of the continuous tensor $z_\Phi(y)$ to the nearest integer.

\vfill
$$\tilde{z}_\Phi(y)[x,y,i] = \lfloor z_\Phi(y)[x,y,i] + 1/2 \rfloor$$

\slide{Rate-Distortion Autoencoders (RDAs)}

Since rounding is not differentiable, at training time we replace rounding by additive noise.

\vfill
{\huge \color{red} $$\Phi^* = \argmin_\Phi\; E_{y \sim \mathrm{\train}}\;E_{\epsilon \sim [-1/2,1/2]^d} \;\left\{\begin{array}{ll} &- \ln p_\Phi(z_\Phi(y) + \epsilon) \\
\\
+ & \lambda \mathrm{Dist}(y,y_\Phi(z_\Phi(y) + \epsilon)) \end{array}\right.$$}

The continuous density $p_\Phi(z)$ is parameterized in a way that guarantees

{\color{red} $$p_\Phi(z) \approx P_\Phi(\tilde{z})$$}

\vfill
At test time we use rounding.

\slide{Rate: Differential Entropy vs. Discrete Entropy}

\bigskip
\centerline{\includegraphics[height=3in]{\images/RateDist6}}

Each point is a rate for an image measured in both differential entropy and discrete entropy.  The size of the rate changes as we change the weight $\lambda$.

\slide{Distortion: Noise vs. Rounding}

\centerline{\includegraphics[height=3in]{\images/RateDist5}}

Each point is a distortion for an image measured in both a rounding model and a noise model.  The size of the distortion changes as we change the weight $\lambda$.

\anaslide{JPEG at 4283 bytes or .121 bits per pixel}

\bigskip
\centerline{\includegraphics[height=5in]{\images/RateDist2}}

\anaslide{JPEG 2000 at 4004 bytes or .113 bits per pixel}

\bigskip
\centerline{\includegraphics[height= 5in]{\images/RateDist3}}

\anaslide{Deep Autoencoder at 3986 bytes or .113 bits per pixel}

\bigskip
\centerline{\includegraphics[height = 5in]{\images/RateDist4}}

\slide{END}

}
\end{document}
