\input ../../SlidePreamble
\input ../../preamble

\newcommand{\solution}[1]{\bigskip {\bf Solution}: #1}

\begin{document}

{\Huge
  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \centerline{\bf SGD with Momentum}
  \vfill
  \vfill

\slideplain{Momentum}

The standard (PyTorch) momentum SGD equations are

\begin{eqnarray*}
  {\color{red} v_t} & {\color{red} =} & {\color{red} \mu v_{t-1} + \eta * \hat{g}_t} \;\;\;\mbox{$\mu$ is typically .9 or .99}\\
  \\
  {\color{red} \Phi_{t+1}} & {\color{red} =} & {\color{red} \Phi_t -  v_t} \\
\end{eqnarray*}

\vfill
Here $v$ is velocity, $0 \leq \mu < 1$ represents friction drag and $\eta \hat{g}$ is the acceleration generated by the gradient force.

\slide{Momentum}

The theory of momentum is generally given in terms of second order structure and total gradients (GD rather than SGD).

\vfill
But second order analyses of are controversial for SDG in very large dimension.

\vfill
Still, momentum is widely used in practice.

\slide{Momentum and Temperature}

\begin{eqnarray*}
  {\color{red} v_t} & {\color{red} =} & {\color{red} \mu v_{t-1} + \eta * \hat{g}_t} \;\;\;\mbox{$\mu$ is typically .9 or .99}\\
  \\
  {\color{red} \Phi_{t+1}} & {\color{red} =} & {\color{red} \Phi_t -  v_t} \\
\end{eqnarray*}

\vfill
We will use a first order analysis to argue that by setting

{\color{red} $$\eta = (1-\mu)B\eta_0$$}

\vfill
the temperature will be essentially determined by $\eta_0$ independent of the choice of the momentum parameter $\mu$ or the batch size $B$.

\slide{Momentum and Temperature}

{\color{red} $$\eta = (1-\mu)B\eta_0$$}

\vfill
Emprical evidence for this setting of $\eta$ is given in

\vfill
{\bf Don't Decay the Learning Rate, Increase the Batch Size}, Smith et al., 2018

\slide{Momentum as a Running Average}
Consider a sequence $x_1$, $x_2$, $x_3$, $\ldots$.
\vfill
For $t \geq N$, consider the average of the $N$ most recent values.
$$\overline{x}_t = \frac{1}{N} \;\; \sum_{k = 0}^{N-1}\; x_{t-k}$$

\vfill
This can be approximated efficiently with
\begin{eqnarray*}
\tilde{x}_0 & = & 0 \\
\\
\tilde{x}_t & = & \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t
\end{eqnarray*}

\slide{Deep Learning Convention for Running Averages}

In deep learning a running average

$$\tilde{x}_t = \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t$$

\vfill
is written as

$$\tilde{x}_t = \beta\tilde{x}_{t-1} + (1-\beta)x_t$$

\vfill
where
$$\beta = 1 - 1/N$$

\vfill
Typical values for $\beta$ are .9, .99 or .999 corresponding to $N$ being 10, 100 or 1000.

\vfill
It will be convenient here to use $N$ rather than $\beta$.

\slide{Momentum as a Running Average}

\begin{eqnarray*}
v_t & = & \mu v_{t-1} + {\color{red} \eta \hat{g}_t} \\
\\
& = & \left(1-\frac{1}{N}\right) v_{t-1} + {\color{red} \frac{1}{N}(N\eta \hat{g}_t)}
\end{eqnarray*}

\vfill
We see that $v_t$ is a running average of $N \eta \hat{g}$.

\slide{Momentum as a Running Average}

\centerline{\color{red} $v_t$ is a running average of $N \eta \hat{g}$.}

\vfill
Alternatively, we can consider a direct running average of the gradient.
{\color{red} $$\tilde{g}_t = \left(1-\frac{1}{N}\right)\tilde{g}_{t-1} + \left(\frac{1}{N}\right) \hat{g}_t$$}

\vfill
The running average of $N\eta\hat{g}$ is the same as $N\eta$ times the running average of $\hat{g}$.  Hence

\vfill
{\color{red} $$v_t = N \eta \tilde{g}_t$$}

\slide{Momentum as a Running Average}

We have now shown that the standard formulation of momentum can be written as

\vfill
\begin{eqnarray*}
\tilde{g}_t & = & \left(1-\frac{1}{N}\right)\tilde{g}_{t-1} + \left(\frac{1}{N}\right) \hat{g}_t \\
\\
\\
\\
\Phi_{t+1} & = &  \Phi_t - N\eta\tilde{g}_t
\end{eqnarray*}

\slide{Total Effect Rule}

We will adopt the rule of thumb that the temperature is determined by the total effect of a single training gradient $g_{t,b}$.


\vfill
Also that ``temperature'' corresponds to the converged loss at fixed learning rate.

\slide{Total Effect Rule}

The effect of $g_{t,b}$ on the batch average $\hat{g}_t$ is $\left(\frac{1}{B}\right)g_{t,b}$.

\vfill
$$\mathrm{Using}\;\;\;\;\; \sum_{i = 0}^\infty \;\frac{1}{N}\left(1 - \frac{1}{N}\right)^i = 1$$

\vfill
we get that the effect of $\hat{g}_t$ on $\sum_{t=0}^\infty \tilde{g}_t$ equals $\hat{g}_t$.

\vfill
So for $\Phi_{t+1} =  \Phi_t - N\eta\tilde{g}_t$ the total effect of $g_{t,b}$ is $(N/B)\eta\; g_{t,b}$.


\slide{Total Effect Rule}

For $\Phi_{t+1} =  \Phi_t - N\eta\tilde{g}_t$ the total effect of $g_{t,b}$ is $(N/B)\eta\; g_{t,b}$.

\vfill
By taking $\eta = \frac{B}{N} \eta_0$ we get that the total effect, and hence the temperature, is determined by $\eta_0$ independent of the choice of $N$ and $B$.

\vfill
For the standard momentum paramenter $\mu = (1 - 1/N)$ this becomes

\vfill
{\color{red} $$\eta = (1-\mu)B \eta_0$$}

\vfill
where $\eta_0$ determines temperature independent of $\mu$ and $B$.

\slide{END}

} \end{document}

