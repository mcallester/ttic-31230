\input ../../SlidePreamble
\input ../../preamble

\begin{document}

{\Huge
  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \centerline{\bf Stationary Distributions of SDEs and Temperature}
  \vfill
  \vfill
  \vfill

\slide{The Stationary Distribution}

\begin{eqnarray*}
\Phi(t + \Delta t) & \approx & \Phi(t) -g(\Phi)\Delta t + \epsilon \sqrt{\Delta t}\;\;\;\;\;\; \epsilon \sim {\cal N}(0,{\color{red} \eta}\Sigma)
\end{eqnarray*}

\vfill
For an SDE we have a stationary continuous density in parameter space.

\vfill
We have a probability mass flow due to the loss gradient and a diffusion probability mass flow proportional to the density gradient.


\slide{The Stationary Distribution with Constant Gradiant Noise}

We consider the one dimensional case --- a single parameter $x$ --- and a probability density $p(x)$.

\vfill
We will assume the stationary distribution is limited to a region where the gradient noise is effectively constant.

\vfill
The gradient flow is equal to $- p(x)g$.

\vfill
The diffusion flow is  $- \frac{1}{2} \;\eta\sigma^2\;dp(x)/dx$ (see the appendix).


\vfill
For a stationary distribution the sum of the two flows is zero giving.

\vfill
{\color{red} $$\frac{1}{2} \eta \sigma^2 \frac{dp}{dx} = - p\frac{d{\cal L}}{dx}$$}


\slide{The 1-D Stationary Distribution}

\vspace{-2ex}
\begin{eqnarray*}
\frac{1}{2} \eta^2 \sigma^2 \frac{dp}{dx} & = & - \eta p\frac{d{\cal L}}{dx} \\
\\
\frac{dp}{p} & = & \frac{-2d{\cal L}}{\eta\sigma^2} \\
\\
\ln p & = & \frac{-2{\cal L}}{\eta \sigma^2} + C \\
\\
{\color{red} p(x)} & = & {\color{red} \frac{1}{Z}\exp\left(\frac{-2{\cal L}(x)}{\eta \sigma^2}\right)}
\end{eqnarray*}

\vfill
We get a Gibbs distribution with $\eta$ as temperature!

\slide{A 2-D Stationary Distribution}

Let $p$ be a probability density on two parameters $(x,y)$.

\vfill
We consider the case where $x$ and $y$ are completely independent with
$${\cal L}(x,y) = {\cal L}(x) + {\cal L}(y)$$

\vfill
For completely independent variables we have
\begin{eqnarray*}
p(x,y) & = & p(x)p(y) \\
\\
&= & \frac{1}{Z} \exp\left(\frac{-2{\cal L}(x)}{ \eta \sigma_x^2} + \frac{-2{\cal L}(y)}{\eta \sigma_y^2}\right)
\end{eqnarray*}

\slide{A 2-D Stationary Distribution}

\begin{eqnarray*}
p(x,y) & = & \frac{1}{Z} \exp\left(\frac{-2{\cal L}(x)}{ \eta \sigma_x^2} + \frac{-2{\cal L}(y)}{\eta \sigma_y^2}\right)
\end{eqnarray*}

\vfill
This is not a Gibbs distribution!

\vfill
It has two different temperature parameters!

\slide{Forcing a Gibbs Distribution}

Suppose we use parameter-specific learning rates $\eta_x$ and $\eta_y$

\begin{eqnarray*}
p(x,y) & = & \frac{1}{Z} \exp\left(\frac{-2{\cal L}(x)}{ \eta_x \sigma_x^2} + \frac{-2{\cal L}(y)}{\eta_y \sigma_y^2}\right)
\end{eqnarray*}

Setting $\eta_x = \eta'/\sigma^2_x$ and $\eta_y = \eta'/\sigma^2_y$ gives

\begin{eqnarray*}
p(x,y) & = & \frac{1}{Z} \exp\left(\frac{-2{\cal L}(x)}{ \eta'} + \frac{-2{\cal L}(y)}{\eta'}\right) \\
\\
& = & \frac{1}{Z} \exp\left(\frac{-2{\cal L}(x,y)}{\eta'}\right)\;\;\;\mathrm{Gibbs!}
\end{eqnarray*}

\slidetwo{The Case of Locally Constant Noise}
{and Locally Quadratic Loss}

In this case we can impose a change of coordinates under which the Hessian is the identity matrix.  So without loss of generality we can take the
Hessian to be the identity.

\vfill
We can consider the covariance matrix of the vectors $\hat{g}$ in the Hessian-normalized coordinate system.

\slidetwo{The Case of Locally Constant Noise}
{and Locally Quadratic Loss}

If we assume constant noise covariance in the neighborhood of the stationary distribution then, in the Hessian normalized
coordinate system, we get a stationary distribution
$$p(\Phi) \propto \exp\left(-\sum_i \frac{2{\Phi_i^2}}{\eta\sigma_i^2}\right)$$

\vfill
where $\Phi_i$ is the projection of $\Phi$ onto to a unit vector in the direction of the $i$th eigenvector of the noise covariance matrix and $\sigma^2_i$
is the corresponding noise eigenvalue (the variance of the $\hat{g}_i$).

\slide{END}

\slide{Appendix: Diffusion Flow}

{\Large
We consider the one dimensional case. In the SDE formalism we move stochastically from $x$ to $x + \epsilon \sqrt{\Delta t}$ with $\epsilon \sim {\cal N}(0,\eta\sigma^2)$
where $\eta$ is the learning rate and $\sigma^2$ is the variance of the random gradients $\hat{g}_{t,b}$.

\vfill
To avoid confusion between different probability densities we will us $\rho(x)$ for the probability mass distribution in $x$ and use $p_\epsilon(\Psi)$
for the probability that $\Psi$ holds under a random draw of $\epsilon$.

}

\slide{Appendix: Diffusion Flow}
{\Large

We move stochastically from $x$ to $x + \epsilon \sqrt{\Delta t}$ with $\epsilon \sim {\cal N}(0,\eta\sigma^2)$

\vfill
This is the same as moving stochastically from $x$ to $x + \epsilon\sqrt{\eta}\sigma\sqrt{t}$ with $\epsilon \sim {\cal N}(0,1)$.

\vfill
The quantity of mass transfer in the time interval $\Delta t$ from values above $x$ to values below $x$ is


\begin{eqnarray*}
& & \int_{z = 0}^\infty  \rho(x + z)\;p_\epsilon(\epsilon\sqrt{\eta}\sigma\sqrt{t} \leq -z) dz  \\
\\
& = & \int_{z = 0}^\infty  \rho(x + z)\;p_\epsilon\left(\epsilon \leq \frac{-z}{\sqrt{\eta}\sigma\sqrt{\Delta t}}\right) dz  \\
\\
& =  & \int_{z = 0}^\infty \rho(x+z)\;\Phi\left(\frac{-z}{\sqrt{\eta}\sigma\sqrt{\Delta t}}\right) dz
\end{eqnarray*}

\vfill
where $\Phi$ is the cummulative function of the Gaussian.
}

\slide{Appendix: Diffusion Flow}
{\Large

The quantity of mass transfer in the time interval $\Delta t$ from values above $x$ to values below $x$ is


\begin{eqnarray*}
&  & \int_{z = 0}^\infty \rho(x+z)\;\Phi\left(\frac{-z}{\sqrt{\eta}\sigma\sqrt{\Delta t}}\right) dz
\end{eqnarray*}

By a change of variables $u = z/(\sqrt{\eta}\sigma\sqrt{t})$ we get

\begin{eqnarray*}
&  & \int_{u = 0}^\infty \rho(x+\sqrt{\eta}\sigma\sqrt{t}\;u)\;\Phi(-u) \sqrt{\eta}\sigma\sqrt{t}\;du
\end{eqnarray*}

\vfill
As $\Delta t \rightarrow 0$ we can use the first order Taylor expansion of the density.

\begin{eqnarray*}
&  & \sqrt{\eta}\sigma\sqrt{t} \int_{u = 0}^\infty \left(\rho(x)+\sqrt{\eta}\sigma\sqrt{t}\;u \frac{d\rho(x)}{dx}\right)\;\Phi(-u)\;du
\end{eqnarray*}
}

\slide{Appendix: Diffusion Flow}
{\Large

\begin{eqnarray*}
&  & \sqrt{\eta}\sigma\sqrt{t} \int_{u = 0}^\infty \left(\rho(x)+\sqrt{\eta}\sigma\sqrt{t}\;u \frac{d\rho(x)}{dx}\right)\;\Phi(-u)\;du \\
\\
& = & \sqrt{\eta}\sigma\sqrt{t}\;\rho(x)\left(\int_{u=0}^\infty \Phi(-u)\;du\right) +  \eta\sigma^2\Delta t \frac{d\rho(x)}{dx} \left(\int_{u=0}^\infty u\Phi(-u) du\right)
\end{eqnarray*}

A similar alanysis shows that the mass transfer from lower values to higher values is

\begin{eqnarray*}
& = & \sqrt{\eta}\sigma\sqrt{t}\;\rho(x)\left(\int_{u=0}^\infty \Phi(-u)\;du\right) -  \eta\sigma^2\Delta t \frac{d\rho(x)}{dx} \left(\int_{u=0}^\infty u\Phi(-u) du\right)
\end{eqnarray*}

\vfill
The net mass transfer in the positive $x$ direction is the second minus the first or

\begin{eqnarray*}
& = & - 2\eta\sigma^2\Delta t \frac{d\rho(x)}{dx} \left(\int_{u=0}^\infty u\Phi(-u) du\right)
\end{eqnarray*}
}

\slide{Appendix: Diffusion Flow}
{\Large

The net mass transfer in the positive $x$ direction is

\begin{eqnarray*}
& & - 2\eta\sigma^2\Delta t \frac{d\rho(x)}{dx} \left(\int_{u=0}^\infty u\Phi(-u) du\right)
\end{eqnarray*}

\vfill
Note that the mass transfer is proportional to $\Delta t$.  Dividing by $\Delta t$ gives the flow per unit time.

\vfill
$$\mbox{Diffusion flow}\;\;= - \alpha \eta\sigma^2 \frac{d\rho(x)}{dx}\;\;\;\;\;\alpha = 2\int_{u=0}^\infty u\Phi(-u) du$$

\vfill
$\alpha$ can be calculated using integration by parts.

\begin{eqnarray*}
\alpha & = & 2 \int_{0}^\infty u \Phi(-u)du \\
& = & \int_{0}^\infty \Phi(-u)du^2 \\
& = & u^2 \Phi(-u)|_{0}^{\infty}+\int_{0}^\infty u^2 \phi(-u)du \;\;\mbox{where $\phi$ is the Gaussian density} \\
& = & \int_{0}^\infty u^2 \phi(-u)du\\
& = & \frac{1}{2}
\end{eqnarray*}

\slide{Appendix: Diffusion Flow}

We now have that the diffusion flow is

$$\mbox{Diffusion flow}\;\;= - \frac{1}{2}\; \eta\sigma^2 \frac{d\rho(x)}{dx}$$

\vfill
For dimension larger than 1 we have

\vfill
$$\mbox{Diffusion flow}\;\;= - \frac{1}{2}\;\eta\Sigma \nabla_x \rho$$

\vfill
Where $\Sigma = E\;(\hat{g} - g)(\hat{g} - g)^\top$ is the covariance matrix of the gradient noise.

\vfill
Here we have derived this from first principle but it also follows from the Fokkerâ€“Planck equation (see Wikipedia).
}

\slide{END}
\end{document}
