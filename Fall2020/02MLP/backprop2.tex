\input ../../SlidePreamble
\input ../../preamble

\begin{document}

{\Huge
  
  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2020}
  \vfill
  \vfill
  \centerline{\bf Backpropagation with Arrays and Tensors}
  \vfill
  \vfill

\slide{Handling Arrays}

\begin{eqnarray*}
  {\color{red} h} & = & \sigma\left(W^0{\color{red} x} - B^0\right) \\
  {\color{red} s} & = & \sigma\left(W^1{\color{red} h} - B^1 \right) \\
  {\color{red} P_\Phi[\hat{y}]} & = & \softmax_{\hat{y}}\;{\color{red} s[\hat{y}]} \\
  {\cal L} & = & - \ln P[y]
\end{eqnarray*}

\vfill
Each array (matrix) {\color{red} $W$} is represented by an object with attributes
{\color{red} $W.\mathrm{value}$} and {\color{red} $W.\mathrm{grad}$}.

\vfill
{\color{red} $W.\mathrm{grad}$} is an array storing {\color{red} $\nabla_W{\cal L}$}.

\vfill
{\color{red} $W.\mathrm{grad}$} has same indeces (same ``shape'') as {\color{red} $W.\mathrm{value}$}.

\slide{Source Code Loops}

\begin{eqnarray*}
   s & = & \sigma\left(Wh - B \right)
\end{eqnarray*}

\vfill
Can be written as

\vfill
$$\begin{array}{lrcl}
\mathrm{for}\;j &  \tilde{s}[j] & = & 0 \\
\\
\mathrm{for}\;j,i &  \tilde{s}[j] & \pluseq &  W[j,i]h[i] \\
\\
\mathrm{for}\; j & s[j] & = & \sigma(\tilde{s}[j] - B[j])
\end{array}$$

\slide{Backpropagation on Loops}
the backpropagation for

$$\begin{array}{lrcl}
\mathrm{for}\;j & {\color{red} s[j]} & = & \sigma(\tilde{s}[j] - B[j])
\end{array}$$

\vfill
is

\vfill
$$\begin{array}{lrcl}
\mathrm{for}\;j & {\color{red} \tilde{s}.\grad[j]} & \pluseq & {\color{red} s.\grad[j]\sigma'(\tilde{s}[j] - B[j])} \\
\\
\mathrm{for}\;j & {\color{red} B.\grad[j]} & \minuseq & {\color{red} s.\grad[j]\sigma'(\tilde{s}[j] - B[j])}
\end{array}$$

\slide{Backpropagation on Loops}
the backpropagation for

$$\begin{array}{lrcl}
\mathrm{for}\;j,i & {\color{red} \tilde{s}[j]} & \pluseq & {\color{red} W[j,i]h[i]}
\end{array}$$

\vfill
is
$$\begin{array}{lrcl}
\mathrm{for}\;j,i & {\color{red} W.\grad[j,i]} & \pluseq & {\color{red} \tilde{s}.\grad[j] h[i]} \\
\\
& {\color{red} h.\grad[i]} & \pluseq & {\color{red} \tilde{s}.\grad[j] W[j,i]} \\
\end{array}$$

\slide{General Tensor Operations}

In practice all deep learning source code can be written unsing scalar assignments and loops over scalar assignments.
For example:

$$\begin{array}{rrcl}
\mathrm{for}\; h,i,j,k & \tilde{Y}[h,i,j] & \;\pluseq\; & A[h,i,k]\;B[h,j,k] \\
\mathrm{for}\; h,i,j & Y[h,i,j] & = & \sigma(\tilde{Y}[h,i,j])
\end{array}$$

\vfill
has backpropagation loops

$$\begin{array}{rrcl}
\mathrm{for}\;h,i,j & \tilde{Y}.\grad[h,i,j] & \pluseq & Y.\grad[h,i,j]\;\;\sigma'(\tilde{Y}.\grad[h,i,j]) \\
\mathrm{for}\;h,i,j,k & A.\grad[h,i,k] & \pluseq & \tilde{Y}.\grad[h,i,j] \;B[h,j,k] \\
\mathrm{for}\;h,i,j,k & B.\grad[h,j,k] & \pluseq & \tilde{Y}.\grad[h,i,j]\;A[h,i,k]
\end{array}$$

\slide{END}
}

\end{document}
