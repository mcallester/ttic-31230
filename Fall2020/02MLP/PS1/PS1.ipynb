{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import edf\n",
    "import mnist_loader\n",
    "\n",
    "train_images, train_labels = mnist_loader.load_mnist(section = 'training', path = 'MNIST')\n",
    "test_images, test_labels = mnist_loader.load_mnist(section = 'testing', path = 'MNIST')\n",
    "\n",
    "plt.imshow(train_images[0], cmap='gray', interpolation = 'nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"we will reshape the 28x28 grayscale images as \n",
    "784-dimensional vectors, which will be the network's inputs\"\"\"\n",
    "\n",
    "train_images = train_images.reshape(len(train_images), -1)\n",
    "test_images = test_images.reshape(len(test_images), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"we manually set the number of input features (784)\n",
    "and number of classes (10) which will be used to\n",
    "define the model\"\"\"\n",
    "\n",
    "nInputs = 784\n",
    "nLabels = 10\n",
    "\n",
    "\"\"\"below, MLPparams is an EDF ParameterPackage whose\n",
    "purpose is to define and store the parameters of the model,\n",
    "while MLPgraph is a function that implements the model's\n",
    "forward pass -- in this case, just a 1-hidden layer MLP\"\"\"\n",
    "\n",
    "class MLPparams(edf.ParameterPackage):\n",
    "    def __init__(self,nInputs, nHiddens, nLabels):\n",
    "        self.first = edf.AffineParams(nInputs,nHiddens)\n",
    "        self.last = edf.AffineParams(nHiddens,nLabels)\n",
    "        \n",
    "def MLPsigmoidgraph(Phi, x):\n",
    "    h = edf.Sigmoid(edf.Affine(Phi.first, x))\n",
    "    return edf.Softmax(edf.Affine(Phi.last, h))\n",
    "\n",
    "\"\"\"we then construct a MLP with 64 hidden units\"\"\"\n",
    "\n",
    "np.random.seed(1234)\n",
    "edf.clear_compgraph()\n",
    "xnode = edf.Input()\n",
    "ynode = edf.Input()\n",
    "nHiddens = 64\n",
    "Phi = MLPparams(nInputs, nHiddens, nLabels)\n",
    "probnode = MLPsigmoidgraph(Phi, xnode)\n",
    "lossnode = edf.LogLoss(probnode, ynode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the following functions are used to train the network.\n",
    "training is done by iterating over mini-batches of size 'batch_size'\n",
    "and updating the model's parameters with SGD\"\"\"\n",
    "\n",
    "def run_epoch(batch_size, data, labels, xnode, ynode, probnode, lossnode=None):\n",
    "    num_samples = len(data)\n",
    "    total_err = 0.0\n",
    "    num_batches = num_samples//batch_size\n",
    "    for i in range(num_batches):\n",
    "        start, end = i*batch_size, (i+1)*batch_size\n",
    "        xnode.value = train_images[start:end]\n",
    "        ynode.value = train_labels[start:end]\n",
    "        edf.Forward()\n",
    "        total_err += np.sum(np.not_equal(np.argmax(probnode.value, axis=1), ynode.value))\n",
    "        if lossnode:\n",
    "            edf.Backward(lossnode)\n",
    "            edf.SGD()\n",
    "        if i>0 and i%400 == 0:\n",
    "            print (\"\\t Batch {}/{}\".format(i, num_batches))\n",
    "    return 100*total_err/num_samples\n",
    "\n",
    "def train(num_epochs, batch_size, xnode, ynode, probnode, lossnode):\n",
    "    train_err_log = []\n",
    "    test_err_log = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, num_epochs))\n",
    "        train_err = run_epoch(batch_size, train_images, train_labels, xnode, ynode, probnode, lossnode)\n",
    "        train_err_log.append(train_err)\n",
    "        print (\"\\t Training Error {:.2f} %\".format(train_err))\n",
    "        test_err = run_epoch(len(test_images), test_images, test_labels, xnode, ynode, probnode)\n",
    "        test_err_log.append(test_err)\n",
    "        print (\"\\t Test Error {:.2f} %\".format(test_err))\n",
    "    return train_err_log, test_err_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"now, we are ready to train the network. we can choose SGD's learning rate\n",
    "by changing edf.learning_rate, which we will set as 0.5 for now.\"\"\"\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "edf.learning_rate = 0.5\n",
    "train_err_log, test_err_log = train(num_epochs, batch_size, xnode, ynode, probnode, lossnode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"we can then plot the error per epoch on the training and test data\"\"\"\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(np.arange(len(test_err_log)), test_err_log, color='red')\n",
    "plt.plot(np.arange(len(train_err_log)), train_err_log, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"next, you have to implement a ReLU activation function ReLU(x) = max(0,x).\n",
    "implement the forward and backward methods of the following class. use the\n",
    "provided EDF source code to understand what each method should do. note that\n",
    "since ReLU has no parameters, it might be helpful to base your implementation\n",
    "on how the Sigmoid EDF CompNode is implemented.\"\"\"\n",
    "\n",
    "class ReLU(edf.CompNode):\n",
    "    def __init__(self, x):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        # implementation goes here\n",
    "\n",
    "    def backward(self):\n",
    "        # implementation goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the code below will plot the output and gradients computed by your\n",
    "implementation of the ReLU component above. check if the plots match\n",
    "ReLU(x) and dReLU/dx(x) as a sanity test of your implementation.\"\"\"\n",
    "\n",
    "values = np.linspace(-2,2,100)\n",
    "edf.clear_compgraph()\n",
    "param = edf.Parameter(values[None, :])\n",
    "output = ReLU(param)\n",
    "\n",
    "edf.Forward()\n",
    "edf.Backward(output)\n",
    "\n",
    "plt.xlabel(\"value\")\n",
    "plt.plot(values, output.value[0], color='red')\n",
    "plt.plot(values, param.grad[0], color='blue')\n",
    "plt.legend(['output', 'grad'], loc='upper left')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"now, train the same network as before but with a ReLU activation\n",
    "in the hidden layer instead of a Sigmoid.\"\"\"\n",
    "\n",
    "def MLPrelugraph(Phi, x):\n",
    "    h = ReLU(edf.Affine(Phi.first, x))\n",
    "    return edf.Softmax(edf.Affine(Phi.last, h))\n",
    "\n",
    "np.random.seed(1234)\n",
    "edf.clear_compgraph()\n",
    "xnode = edf.Input()\n",
    "ynode = edf.Input()\n",
    "nHiddens = 64\n",
    "Phi = MLPparams(nInputs, nHiddens, nLabels)\n",
    "probnode = MLPrelugraph(Phi, xnode)\n",
    "lossnode = edf.LogLoss(probnode, ynode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "edf.learning_rate = 0.5\n",
    "train_err_log, test_err_log = train(num_epochs, batch_size, xnode, ynode, probnode, lossnode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(np.arange(len(test_err_log)), test_err_log, color='red')\n",
    "plt.plot(np.arange(len(train_err_log)), train_err_log, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"now, implement the Tanh activation function by filling the missing\n",
    "code in the forward and backward methods below. it might be helpful to derive\n",
    "a relationship between Tanh and Sigmoid so that you can re-use parts of \n",
    "EDF's Sigmoid code.\"\"\"\n",
    "\n",
    "class Tanh(edf.CompNode):\n",
    "    def __init__(self, x):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        # implementation goes here\n",
    "\n",
    "    def backward(self):\n",
    "        # implementation goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"again, make sure that the output and gradients plotted below are correct.\"\"\"\n",
    "\n",
    "values = np.linspace(-5,5,100)\n",
    "edf.clear_compgraph()\n",
    "param = edf.Parameter(values[None, :])\n",
    "output = Tanh(param)\n",
    "\n",
    "edf.Forward()\n",
    "edf.Backward(output)\n",
    "\n",
    "plt.xlabel(\"value\")\n",
    "plt.plot(values, output.value[0], color='red')\n",
    "plt.plot(values, param.grad[0], color='blue')\n",
    "plt.legend(['output', 'grad'], loc='upper left')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"you can then train a network with a Tanh activation function instead\n",
    "of Sigmoid/ReLU.\"\"\"\n",
    "\n",
    "def MLPtanhgraph(Phi, x):\n",
    "    h = Tanh(edf.Affine(Phi.first, x))\n",
    "    return edf.Softmax(edf.Affine(Phi.last, h))\n",
    "\n",
    "np.random.seed(1234)\n",
    "edf.clear_compgraph()\n",
    "xnode = edf.Input()\n",
    "ynode = edf.Input()\n",
    "nHiddens = 64\n",
    "Phi = MLPparams(nInputs, nHiddens, nLabels)\n",
    "probnode = MLPtanhgraph(Phi, xnode)\n",
    "lossnode = edf.LogLoss(probnode, ynode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "edf.learning_rate = 0.5\n",
    "train_err_log, test_err_log = train(num_epochs, batch_size, xnode, ynode, probnode, lossnode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(np.arange(len(test_err_log)), test_err_log, color='red')\n",
    "plt.plot(np.arange(len(train_err_log)), train_err_log, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"next, let's check how changing the learning rate affects\n",
    "the model's train and test error. you should implement a way\n",
    "to collect the network's final train/test errors for each of\n",
    "the learning rates in the array below. you should do this for\n",
    "MLPs with sigmoid and relu activations, and select the best\n",
    "learning rate for each of the two networks (w/ sigmoid \n",
    "and w/ relu activations)\"\"\"\n",
    "\n",
    "\"\"\"add code below to train MLPs with sigmoid activations.\n",
    "your code should populate the arrays train_err_per_lr and\n",
    "test_err_per_lr, such that they contain the train and test\n",
    "errors of models trained with each learning rate in the\n",
    "learning_rates arrays, i.e. train_err_per_lr[1] should contain\n",
    "the final train error of a sigmoid MLP trained with a learning\n",
    "rate of 1.0\"\"\"\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rates = [5.0, 1.0, 0.5, 0.1]\n",
    "\n",
    "train_err_per_lr = []\n",
    "test_err_per_lr = []\n",
    "\n",
    "# implementation goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sigmoid_lr = # implementation goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"learning rate\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(learning_rates, test_err_per_lr, color='red')\n",
    "plt.plot(learning_rates, train_err_per_lr, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"now, repeat the experiment above but with a MLP with\n",
    "a ReLU activation functions in the hidden layer\"\"\"\n",
    "\n",
    "train_err_per_lr = []\n",
    "test_err_per_lr = []\n",
    "\n",
    "# implementation goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_relu_lr = # implementation goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"learning rate\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(learning_rates, test_err_per_lr, color='red')\n",
    "plt.plot(learning_rates, train_err_per_lr, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"finally, we will check how the number of hidden layers affects\n",
    "the model's performance. start by re-implementing MLPparams such that\n",
    "it supports multiple layers (note the new nLayers argument). each of the\n",
    "nLayers-1 hidden layers should have nHiddens neurons.\"\"\"\n",
    "\n",
    "class MLPparams(edf.ParameterPackage):\n",
    "    def __init__(self,nInputs, nHiddens, nLabels, nLayers):\n",
    "        # implementation goes here\n",
    "        \n",
    "\"\"\"you should also re-implement MLPsigmoidgraph and MLPsigmoidgraph\n",
    "so that they both have support for multiple layers, following your\n",
    "re-implementation of MLPparams above.\"\"\"\n",
    "        \n",
    "def MLPsigmoidgraph(Phi, x):\n",
    "    # implementation goes here\n",
    "\n",
    "def MLPsigmoidgraph(Phi, x):\n",
    "    # implementation goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"similarly to the exploration with different learning rates,\n",
    "you will see how the depth of the network affects its performance,\n",
    "first for a sigmoid network. fill the missing code to populate\n",
    "train_err_per_depth and test_err_per_depth accordingly\"\"\"\n",
    "\n",
    "edf.learning_rate = best_sigmoid_lr\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "num_layers = [2, 4, 6]\n",
    "\n",
    "train_err_per_depth = []\n",
    "test_err_per_depth = []\n",
    "\n",
    "# implementation goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"depth\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(num_layers, test_err_per_depth, color='red')\n",
    "plt.plot(num_layers, train_err_per_depth, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"repeat the above experiment but for a ReLU MLP\"\"\"\n",
    "\n",
    "edf.learning_rate = best_relu_lr\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "num_layers = [3, 5, 10]\n",
    "\n",
    "train_err_per_depth = []\n",
    "test_err_per_depth = []\n",
    "\n",
    "# implementation goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"learning rate\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(num_layers, test_err_per_depth, color='red')\n",
    "plt.plot(num_layers, train_err_per_depth, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
