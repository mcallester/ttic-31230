\input ../../SlidePreamble
\input ../../preamble


\begin{document}

{\Huge


\centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
\bigskip
\centerline{David McAllester, Autumn 2020}

\vfill
\centerline{\bf Implicit Regularization}
\vfill
\vfill

\slide{Implicit Regularization}

Any stochastic learning algorithm, such as SGD, determines a stochastic mapping from training data to models.

\vfill
The algorithm, especially with early stopping, can implicitly incorporate a preference or bias for models.

\slide{Implicit Regularization in Linear Regression}

Linear regression with many more parameters than data points
has many solutions.

\vfill
But SGD converges to the minimum norm solution.

\slide{Implicit Regularization in Linear Regression}

For linear regression SGD maintains the invariant that $\Phi$ is a linear combination of the (small number of) training vectors.

\vfill
Any zero-loss (squared loss) solution can be projected on the span of training vectors to give a smaller (or no larger) norm solution.

\vfill
It can be shown that when the training vectors are linearly independent any zero loss solution in the span of the training vectors is a least-norm solution.

\slide{Implicit Regularization of SGD}

In a labeling problem a model with parameters $\Phi$ defines a model propbability $P_\Phi(y|x)$.

\vfill
This defines a log loss $-\ln P_\Phi(y|x)$ on which we do gradient descent.

\vfill
Let {\color{red} $\mathrm{SGD}\!\brackets{P_\Phi,\Phi_{\mathrm{Init}},\train}$} be the vector that results from running SGD on model $P_\Phi$ with initial parameters $\Phi_{\mathrm{Init}}$
using training data $\train$ (and a fixed set of hyperparameters, learning rate schedule, and fixed order in which training instances are considered,
and fixed number of iterations).


\slide{Implicit Regularization of SGD}

To get a generalization bound when learning a continuous parameter vector we add Gaussian noise to simulate limited precision of the real numbers.

$$\Phi' = \mathrm{SGD}\!\brackets{P_\Phi,\Phi_{\mathrm{Init}},\mathrm{Train}} + \epsilon$$

\vfill
The algorithm defines an {\bf implicit prior:}
$$p\!\parens{\Phi'\;|\;P_\Phi,\Phi_{\mathrm{Init}},\pop} = E_{\parens{\mathrm{Train} \sim \pop^N}}\;\;p\!\parens{\Phi'\;|\;P_\Phi,\Phi_{\mathrm{Init}},\train}$$

\vfill
The implicit prior $p\!\parens{\Phi'\;|\;P_\Phi,\Phi_{\mathrm{Init}},\pop}$ is a valid prior!  It does not depend on training data!

\slide{Implict Priors: the General Case}

Let $A$ be any algorithm mapping a training set $\train$ to a probability density $q_{A,\train}(\Phi')$ over model parameters $\Phi'$.

\vfill
The implicit prior defined by algorithm $A$ and the given population distribution is

$$p_{A,\pop}(\Phi') = E_{\parens{\mathrm{Train} \sim \pop^{N}}} \;\;q_{A,\train}(\Phi')$$

\slide{A PAC-Bayes Analysis of Implicit Regularization}

\begin{eqnarray*}
{\cal L}(q_{A,\train}) & = & E_{\tuple{x,y} \sim {\color{red}  \pop}, \;\;\Phi' \sim q_{A,\train}}\;{\cal L}(\Phi',x,y) \\
\\
\hat{\cal L}(q_{A,\train}) & = & E_{\tuple{x,y} \sim {\color{red} \train}, \;\;\Phi' \sim q_{A,\train}}\;{\cal L}(\Phi',x,y)
\end{eqnarray*}

\slide{A PAC-Bayes Analysis of Implicit Regularization}

With probability at least $1-\delta$ over the draw of $\train$ we have
\vfill
{\huge
$${\cal L}(q_{A,\train}) \leq \frac{10}{9}\parens{ \hat{\cal L}(q_{A,\train}) + \frac{5\lmax}{N_\mathrm{Train}}\parens{KL(q_{A,\train},p_{A,\pop})+ \ln\frac{1}{\delta}}}$$
}
\vfill
There is no obvious way to calculate this guarantee.

\vfill
However, it can be shown that $p_{A,\pop}$ is the optimal PAC-Bayeisan prior for algorithm $A$ run on data drawn from $\pop$.

\slide{END}

}
\end{document}
