\input ../../SlidePreamble
\input ../../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2020}

    \vfill
  \centerline{\bf Trainability:}
  \vfill
  \centerline{\bf ReLU, Batch Normalization, Initialization,}
  \vfill
  \centerline{\bf and Residual Connections (ResNet)}
  \vfill
  \vfill

\ignore{
\slide{Universality Assumption}

We often assume DNNs are universally expressive (can model any function) and trainable (the desired function can be found by SGD).

\vfill
Universal trainability is clearly false but can still usefully guide architecture design.

\slide{Universality Assumption: Expressiveness}

DNNs generalize digital circuits.

\vfill
Consider Boolean Values $P,Q$ --- numbers that are either close to 0 or close to 1.
\begin{eqnarray*}
P \wedge Q & \approx & \sigma(100*P + 100* Q -150) \\
\\
P \vee Q & \approx & \sigma(100*P + 100* Q -50) \\
\\
\neg P & \approx & \sigma(100*(1-P) - 50)
\end{eqnarray*}
}

\slide{Trainability}

The main issue in making deep neural networks trainable is maintaining meaningful gradients.

\vfill
There are various difficulties.

\slide{Activation Function Saturation}

Consider the sigmoid activation function $1/(1+ e^{-x})$.

\vfill
\centerline{\includegraphics[width= 4.0in]{\images/sigmoid2}}


\vfill
The gradient of this function is quite small for $|x| > 4$.

\vfill
In deep networks backpropagation can go through many sigmoids and
we get ``vanishing gradients''.

\slide{The Rectified Linear Unit Activation Function (ReLU)}

$\mathrm{ReLU}(x) = \max(x,0)$

\vfill
\centerline{\includegraphics[width= 4.0in]{\images/relu}}

\vfill
The activation function $\mathrm{ReLU}(x)$ does not saturate for $x > 0$.

\slide{Repeated Multiplication by Network Weights}

Consider a deep CNN.

$$L_{i+1} = \mathrm{ReLU}(\mathrm{Conv}(\Phi_i,L_i))$$

\vfill
For $i$ large, $L_i$ has been multiplied by many weights.

\vfill
If the weights are small then the neuron values, and hence the weight gradients, decrease exponentially with depth. {\bf Vanishing Gradients.}

\vfill
If the weights are large, and the activation functions do not saturate, then the neuron values, and hence the weight gradients,
increase exponentially with depth. {\bf Exploding Gradients.}

\slide{Repeated Multiplication by Network Weights}

The problem of repeated multiplication by network weights can be addressed with careful initialization.

\vfill
We want an initialization for which the values stay in the active regions of the activation functions --- zero mean and unit variance.

\slide{Initialization}


Consider a linear threshold unit

\vfill
$$y[j] = \sigma(W[j,I]x[I] - B[j])$$

\vfill
We want the scalar $y[j]$ to have zero mean and unit variance.

\vfill
Xavier initialization initializes $B[j]$ to zero and randomly draws $W[j,i]$ from a uniform distribution on $\left(-\sqrt{3/I},\;\sqrt{3/I}\right)$.

\vfill
Assuming $x[i]$ has zero mean and unit variance, this gives zero mean and unit variance for $W[j,I]x[I]$.

\slide{Batch Normalization}

We can also enforce zero mean, unit variance, values dynamically with normalization layers.

\vfill
In vision networks this is most commonly done with Batch Normalization.

\slide{Batch Normalization}
Given a tensor $x[b,j]$ we define $\tilde{x}[b,j]$ as follows.

\begin{eqnarray*}
  \hat{\mu}[j] & = & \frac{1}{B} \sum_b\;x[b,j] \\
  \\
  \\
  \hat{\sigma}[j] & = & \sqrt{\frac{1}{B-1} \sum_b (x[b,j]-\hat{\mu}[j])^2} \\
  \\
  \\
  \tilde{x}[b,j]& = & \frac{x[b,j] - \hat{\mu}[j]}{\hat{\sigma}[j]}
\end{eqnarray*}


\vfill
At test time a single fixed estimate of $\mu[j]$ and $\sigma[j]$ is used.

\slide{Spatial Batch Normalization}

For CNNs we convert a tensor $x[b,x,y,j]$ to $\tilde{x}[b,x,y,j]$ as follows.

\begin{eqnarray*}
  \hat{\mu}[j] & = & \frac{1}{BXY} \sum_{b,x,y}\;x[b,x,y,j] \\
  \\
  \\
  \hat{\sigma}[j] & = & \sqrt{\frac{1}{BXY-1} \sum_{b,x,y} (x[b,x,y,j]-\hat{\mu}[j])^2} \\
  \\
  \\
  \tilde{x}[b,x,y,j]& = & \frac{x[b,x,y,j] - \hat{\mu}[j]}{\hat{\sigma}[j]}
\end{eqnarray*}

\slide{Adding an Affine Transformation}

$$\breve{x}[b,x,y,j] = \gamma[j] \tilde{x}[b,x,y,j] + \beta[j]$$

\vfill
Here $\gamma[j]$ and $\beta[j]$ are parameters of the batch normalization.

\vfill
This allows the batch normlization to learn an arbitrary affine transformation (offset and scaling).

\vfill
It can even undo the normaliztion.

\slide{Batch Normalization}

Batch Normalization appears to be generally useful in CNNs but is not always used.

\vfill
Not so successful in RNNs.

\vfill
It is typically used just prior to a nonlinear activation function.

\vfill
It is intuitively justified in terms of ``internal covariate shift'':
as the inputs to a layer change the zero mean unit variance property underlying Xavier initialization are maintained.

\slide{Residual Connections (ResNet)}

\vfill
\includegraphics[width= 2.5in]{\images/resnet}
\hfill \begin{minipage}[b]{4in}
  A residual connection produces the sum of the previous layer and the new layer.

  \bigskip
  The residual connection connects input to output directly and hence preserves gradients.

  \bigskip
  Residual Connections were introduced in late 2015 (Kaiming He et al.) and are now standard in all areas of deep learning.
\end{minipage}

\anaslideplain{A Simple Residual Connection in CNNs (stride 1)}

\medskip
\begin{eqnarray*}
L_{\color{red} \ell+1}[B,X,Y,J] & = & \mathrm{Conv}(W_{\color{red} \ell+1}[X,Y,J,J],B_{\color{red} \ell+1}[J],L_{\color{red} \ell}[B,X,Y,J]) \\
\\
& & + \;\; L_{\color{red} \ell}[B,X,Y,J]
\end{eqnarray*}

\vfill (Recall that we use capital letter indices to denote entire tensors and lower case letters for particular indeces.)

\slide{Multiple Convolutions Between Additions}

\begin{eqnarray*}
L_{\color{red} \ell+1}[B,X,Y,J] & = & \mathrm{Conv}(W_{\color{red} \ell+1}[X,Y,J,J],B_{\color{red} \ell+1}[J],L_{\color{red} \ell}[B,X,Y,J]) \\
\\
L_{\color{red} \ell+2}[B,X,Y,J] & = & \mathrm{Conv}(W_{\color{red} \ell+2}[X,Y,J,J],B_{\color{red} \ell+2}[J],L_{\color{red} \ell+1}[B,X,Y,J]) \\
\\
L_{\color{red} \ell+3} & = & L_{\color{red} \ell} + L_{\color{red} \ell+2}
\end{eqnarray*}

\slide{ResNet32}

\centerline{\includegraphics[height= 5.5in]{\images/ResNetStack} {\large [Kaiming He]}}

\anaslideplain{Reducing Spacial Dimension}

\medskip
\begin{eqnarray*}
L_{\color{red} \ell+1} & = & \mathrm{Conv}(W_{\color{red} \ell+1},B_{\color{red} \ell+1},L_{\color{red} \ell}) \\
\\
L_{\color{red} \ell+2} & = & \mathrm{Conv}(W_{\color{red} \ell+2},B_{\color{red} \ell+2},L_{\color{red} \ell+1}) \\
\\
L_{\color{red} \ell+3} & = & L_{\color{red} \ell} + L_{\color{red} \ell+2}
\end{eqnarray*}

\vfill All these layers have the same shape.

\anaslide{Reducing Spacial Dimension}

\medskip
\begin{eqnarray*}
L_{\color{red} \ell+1} & = & \mathrm{Conv}(W_{\color{red} \ell+1},B_{\color{red} \ell+1},L_{\color{red} \ell},\;\mathrm{stride} \;s) \\
\\
L_{\color{red} \ell+2} & = & \mathrm{Conv}(W_{\color{red} \ell+2},B_{\color{red} \ell+2},L_{\color{red} \ell+1}) \\
\\
L_{\color{red} \ell+3} & = & \tilde{L}_{\color{red} \ell} + L_{\color{red} \ell+2}
\end{eqnarray*}

\vfill
where
\begin{eqnarray*}
X_{\color{red} \ell+1} & = & X_{\color{red} \ell}/s \\
Y_{\color{red} \ell+1} & = & Y_{\color{red} \ell}/s \\
J_{\color{red} \ell+1} & \geq &  J_{\color{red} \ell}
\end{eqnarray*}

\anaslide{Reducing Spacial Dimension}

\medskip
\begin{eqnarray*}
L_{\color{red} \ell+1} & = & \mathrm{Conv}(W_{\color{red} \ell+1},B_{\color{red} \ell+1},L_{\color{red} \ell},\;\mathrm{stride} \;s) \\
\\
L_{\color{red} \ell+2} & = & \mathrm{Conv}(W_{\color{red} \ell+2},B_{\color{red} \ell+2},L_{\color{red} \ell+1}) \\
\\
L_{\color{red} \ell+3} & = & \tilde{L}_{\color{red} \ell} + L_{\color{red} \ell+2}
\end{eqnarray*}

\vfill
\begin{eqnarray*}
\tilde{L}_{\color{red} \ell}[b,x,y,j] & = & \left\{\begin{array}{ll} L_{\color{red} \ell}[b,s*x,s*y,j] & \mbox{for $j < J_{\color{red} \ell}$} \\ 0 & \mbox{otherwise} \end{array}\right.
\end{eqnarray*}



\slide{ResNet32}

\centerline{\includegraphics[height= 5.5in]{\images/ResNetStack} {\large [Kaiming He]}}


\slideplain{Deeper Versions use Bottleneck Layers}
We reduce the number of neurons at each position in an intermediate ``bottleneck'' layer.

\begin{eqnarray*}
L_{\color{red} \ell+1} & = & \mathrm{Conv}(W_{\color{red} \ell+1},B_{\color{red} \ell+1},L_{\color{red} \ell})) \\
L_{\color{red} \ell+2} & = & \mathrm{Conv}(W_{\color{red} \ell+2},B_{\color{red} \ell+2},L_{\color{red} \ell+1}) \\
L_{\color{red} \ell+3} & = & \mathrm{Conv}(W_{\color{red} \ell+3},B_{\color{red} \ell+3},L_{\color{red} \ell+2}) \\
L_{\color{red} \ell+3} & = & \tilde{L}_{\color{red} \ell} + L_{\color{red} \ell+3}
\end{eqnarray*}

\vfill
Here $J_{\color{red} \ell+3} = J_{\color{red} \ell}$ but $J_{\color{red} \ell+1} = J_{\color{red} \ell+2} < J_{\color{red} \ell}$.

\vfill
Reducing the number of neurons greatly reduces the size of the convolution weight tensor which has size $\Delta X \times \Delta Y \times J_{\color{red} \ell} \times J_{\color{red} \ell+1}$.

\slide{A General Residual Connection}

$${\color{red} y = \tilde{x} + R(x)}$$

\vfill
Where $\tilde{x}$ is either $x$ or a version of $x$ adjusted to match the shape of $R(x)$.

\ignore{
\slide{DenseNet}

We compute a residual $R[b,x,y,J_R]$ and then simply concatenate the residual onto the previous layer.

\vfill
$$\mbox{for}\;b,x,y\;\;L_{\color{red} \ell + 1}[b,x,y,J_{\color{red} \ell} + J_{\color{red} R}] = L_\ell[b,x,y,J_{\color{red} \ell}];R[b,x,y,J_{\color{red} R}]$$

\vfill
The number $J_R$ of new features can be can be relatively small.
}

\slideplain{ResNet Simplicity}

\centerline{\includegraphics[height= 4.0 in]{\images/ResNetStack} {\large [Kaiming He]} \includegraphics[width = 5.0in]{\images/inception1}}

\slide{ResNet Power}

ResNet gives powerful image classification.

\vfill
ResNet is used in folding proteins.

\vfill
ResNet is the network used in AlphaZero for Go, Chess and Shogi.

\vfill
Residual connections are now universal in all forms of deep models such as RNNs and Transformers in language processing.

\slide{END}

}
\end{document}
