\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge

\centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
\bigskip
\centerline{David McAllester, Autumn 2023}
\vfill
\centerline{\bf Continuous Time Models of SGD}
\vfill
\vfill
\centerline{\bf Gradient Flow}
\vfill
\centerline{\bf The Diffusion SDE}
\vfill
\centerline{\bf The Langevin SDE}
\vfill
\centerline{\bf General SDEs}
\vfill
\centerline{\bf The SGD SDE}

\slide{Gradient Flow}

Gradient flow is a non-stochastic ({\color{red} deterministic}) model of {\color{red} stochastic} gradient descent (SGD).

\vfill
Gradient flow is defined by the {\color{red} total gradient} differential equation

$${\color{red} \frac{d \Phi}{d t} = - g(\Phi) \;\;\;\;\;\; g(\Phi) = \nabla_\Phi\; E_{(x,y) \sim \mathrm{Train}}\;{\cal L}(\Phi,x,y)}$$

\vfill
We let $\Phi(t)$ be the solution to this differential equation satisfying $\Phi(0) = \Phi_{\mathrm{init}}$.

\slide{Gradient Flow}
$${\color{red} \frac{d \Phi}{d t} = - g(\Phi)}$$

\vfill
For small values of $\Delta t$ this differential equation can be approximated by

\vfill
$${\color{red} \Delta \Phi = - g(\Phi)\Delta t}$$

\slide{Time as the Sum of the Learning Rates}

Consider the update.


{\color{red} $$\Delta \Phi = - g\Delta t$$}

\vfill
Here $\Delta t$ has both a natural interpretation as time in a numerical simulation of the flow differential equation.

\vfill
But it also has a natural interpretation as a learning rate.

\vfill
This leads to interpreting the sum of the learning rates as ``time'' in SGD.

\slide{Gradient Flow and SGD}
Consider a sequence of model parameters $\Phi_1$, $\ldots$, $\Phi_N$ produced by SGD with
$$\Phi_{i+1} = \Phi_i - \eta \hat{g}_i$$
and where $\hat{g}_i$ is the gradient of the $i$th randomly selected training point.

\vfill
Take $\eta \rightarrow 0$ and $N \rightarrow \infty$ using $N = t/\eta$.  We will show that in this limit for SGD we have that $\Phi_N$ converges to $\Phi(t)$
as defined by gradient flow.

\slide{Gradient Flow and SGD}
For $\Phi_{i+1} = \Phi_i - \eta \hat{g}_i$ we divide $\Phi_1$, $\ldots$, $\Phi_N$ into $\sqrt{N}$ blocks.
$$(\Phi_1,\dots,\Phi_{\sqrt{N}})\;(\Phi_{\sqrt{N}+1},\ldots,\Phi_{2\sqrt{N}})\;\cdots\;(\Phi_{T-\sqrt{N}+1},\ldots,\Phi_N)$$

\vfill
For $\eta \rightarrow 0$ and $N = t/\eta$ we have $\eta\sqrt{N} \rightarrow 0$ which implies
$$\Phi_{\sqrt{N}} \sim \Phi_0 - \eta\sqrt{N} g$$
where $g$ is the average (non-stochastic)
gradient.

\vfill
Since the gradients within each block become non-stochastic, we are back to gradient flow.


\slide{Diffusion}
Consider a discrete-time process $z(0),z(1),z(2),z(3),\ldots$ with $z(n)\in \mathbb{R}^d$ defined by
\begin{eqnarray*}
  z(0) & = & y,\;\;\;y \sim \popd(y) \\
  z(n+1) & = & z(n) + \sigma\epsilon,\;\;\;\epsilon \sim {\cal N}(0,I)
\end{eqnarray*}

\vfill
We can sample from $z(n)$ using

\begin{eqnarray*}
  z(0) & = & y,\;\;\;y \sim \popd(y) \\
  z(n) & = & z(0) + \sigma\epsilon\sqrt{n} ,\;\;\;\epsilon \sim {\cal N}(0,I)
\end{eqnarray*}

\slide{Diffusion}
Fix a numerical time step $\Delta t$ and consider a discrete-time process $z(0)$, $z(\Delta t)$, $z(2\Delta t)$, $\ldots$

{\huge
\begin{eqnarray*}
  z(0) & = & y,\;\;\;y \sim \popd(y) \\
  \\
  z(t+\Delta t) & = & z(t) + \sigma\epsilon\sqrt{\Delta t},\;\;\;\epsilon\sim {\cal N}(0,I) \\
\end{eqnarray*}
}
We now take the limit of this numerical simulation as $\Delta t \rightarrow 0$.

\vfill
This limit defines a probability measure on the space of functions $z(t)$.

\slide{The Diffusion SDE}

$$z(t+\Delta t) =  z(t) + \sigma\epsilon\sqrt{\Delta t},\;\;\;\epsilon \sim {\cal N}(0,I)$$

\vfill
For simple diffusion (Brownian motion) this equation holds for any continuous $t \geq 0$
and $\Delta t \geq 0$.

\slide{The Langevin SDE}

Consider gradient flow.

\begin{eqnarray*}
\frac{d\Phi(t)}{dt} & = & - g(\Phi) \\
\\
g(\Phi) & = & \nabla_\Phi\;{\cal L}(\Phi) \\
\\
{\cal L}(\Phi) & = & E_{(x,y) \sim \pop}\;{\cal L}(\Phi,x,y)
\end{eqnarray*}

\slide{The Langevin SDE}

In the Langevin SDE we add Gaussian noise to gradient flow.

\begin{eqnarray*}
\Phi(t + \Delta t) & = & \Phi(t) - g\Delta t + \sigma \epsilon \sqrt{\Delta t},\;\;\;\epsilon \sim {\cal N}(0,I)
\end{eqnarray*}

\vfill
We will show that the stationary distribution of Langevin Dynamics models a Bayesian posterior probability distrbution on
the model parameters where $\sigma$ acts as a temperature parameter.


\slide{The Langevin SDE}

\begin{eqnarray*}
\Phi(t + \Delta t) & = & \Phi(t) - g(\Phi)\Delta t + \sigma\epsilon \sqrt{\Delta t},\;\;\;\epsilon \sim {\cal N}(0,I)
\end{eqnarray*}

Let $p(\Phi)$ be a probability density on the parameter space $\Phi$.

\vfill
The density $p(\Phi)$ defines a gradient flow and a diffusion flow.

\begin{eqnarray*}
\mbox{gradient flow} & = & - p(\Phi)g(\Phi) \\
\\
\mbox{diffusion flow} & = & - \frac{1}{2} \;\sigma^2\;\nabla_\Phi\; p(\Phi)
\end{eqnarray*}

\vfill
The expression for the diffusion flow follows from the Fokker-Plank equation. A derivation of the diffusion flow expression from first princple is given in the appendix.

\slide{The Langevin SDE}

\begin{eqnarray*}
\mbox{gradient flow} & = & - p(\Phi)g(\Phi) \\
\\
\mbox{diffusion flow} & = & - \frac{1}{2} \;\sigma^2\;\nabla_\Phi(p(\Phi))
\end{eqnarray*}

\vfill
For the stationary distribution these two flows cancel each other out.
In one dimension we have

$$\frac{1}{2} \sigma^2 \nabla_\Phi\; {p} = - p\nabla_\Phi {\cal L}$$

\slide{The Langevin Stationary Distribution}

\vspace{-2ex}
\begin{eqnarray*}
\frac{1}{2} \sigma^2 \nabla_\Phi\; {p} & = & - p\nabla_\Phi {\cal L} \\
\\
\frac{1}{2} \sigma^2 \frac{\nabla_\Phi\;p}{p} & = & - \nabla_\Phi {\cal L} \\
\\
\nabla_\Phi \left(\frac{1}{2} \sigma^2 \ln p\right) & = &  \nabla_\Phi (- {\cal L}) \\
\\
\frac{1}{2} \sigma^2 \ln p & = & - {\cal L} + C \\
\\
{\color{red} p(\Phi)} & {\color{red} =} & {\color{red} \frac{1}{Z}\exp\left(\frac{-2{\cal L}(\Phi)}{\sigma^2}\right)}
\end{eqnarray*}

\slide{A Bayesian Interpretation of Langevin Dynamics}

{\huge
$$\train = (x_1,y_1),\ldots,(x_n,y_n)$$

\vfill
The parameters $\Phi$ determine $p_\Phi(y|x)$.

\begin{eqnarray*}
p(\Phi|\train) & = & \frac{p(\Phi)p(\train|\Phi)}{p(\train)} \\
\\
\\
& = & \frac{p(\Phi)p(x_1,\ldots,x_n)p_\Phi(y_1,\ldots,y_n|x_1,\ldots,x_n)}{p(x_1,\ldots,x_n)p_\Phi(y_1,\ldots y_n|x_1,\ldots x_n)} \\
\\
\\
& = & \frac{p(\Phi)p_\Phi(y_1,\ldots,y_n|x_1,\ldots,x_n)}{p(y_1,\ldots y_n|x_1,\ldots x_n)}
\end{eqnarray*}
}

\slide{A Bayesian Interpretation of Langevin Dynamics}

$$\train = (x_1,y_1),\ldots,(x_n,y_n)$$

\vfill
{\huge
\begin{eqnarray*}
p(\Phi|\train) & = & \frac{p(\Phi)p_\Phi(y_1,\ldots,y_n|x_1,\ldots,x_n)}{p(y_1,\ldots y_n|x_1,\ldots x_n)}
\end{eqnarray*}
}

\vfill
The denominator does does not depend on $\Phi$ which implies

$$p(\Phi|\train) \propto p(\Phi)\prod_i p_\Phi(y_i|x_i)$$


\slide{A Bayesian Interpretation of Langevin Dynamics}

{\huge
\begin{eqnarray*}
p(\Phi|\train) & \propto & p(\Phi)\prod_i p_\Phi(y_i|x_i) \\
\\
\ln p(\Phi|\train) & = & \frac{1}{n} \sum_i \ln p_\Phi(y_i|x_i) + \frac{1}{n}\ln p(\Phi) + C
\end{eqnarray*}

Defining

$${\cal L}(\Phi) = \frac{1}{n} \sum_i -\ln p_\Phi(y_i|x_i) - \frac{1}{n} \ln p(\Phi)$$

gives

{\color{red} $$p(\Phi|\train) = \frac{1}{Z}\exp\left(-{\cal L}(\Phi)\right)$$}
}






\slide{A Bayesian Interpretation of Langevin Dynamics}

$$p(\Phi|\train) = \frac{1}{Z}e^{-{\cal L}(\Phi)}$$

\vfill
$$p_{\mathrm{Langevin}}(\Phi) = \frac{1}{Z}\exp\left(\frac{-2{\cal L}(\Phi)}{\sigma^2}\right)$$

\vfill
Setting $\sigma^2 = 1/2$ gives

\vfill
$$p_{\mathrm{Langevin}}(\Phi) = p(\Phi|\train)$$

\slide{A General SDE}

{\huge
\begin{eqnarray*}
x(t + \Delta t) &  =  & x(t) + \mu(x,t)\Delta t + \sigma(x,t)\epsilon \sqrt{\Delta t},\;\;\;\epsilon \sim {\cal N}(0,I)\;\;\;(1)
 \end{eqnarray*}
}
\vfill
Here $\sigma(x,t)$ is a matrix.

\vfill
This is conventionally written as

$$dx = \mu(x,t)dt + \sigma(x,t)dB\;\;\;\;(2)$$

\vfill
where $B$ denotes a Weiner process (simple diffusion, aka Brownian motion)

\vfill
I find (1) more intuitive than (2) but they are the same thing.

\slide{The SGD SDE}

\vfill
We now consider SGD

$$\Phi_{i+1} = \Phi_i - \eta\hat{g}_i$$

\vfill
We consider $\Phi_i$ and $\Phi_{i+N}$ with $N$ small enough that

$$\Phi_{i+N} \approx \Phi_i$$.

\slide{Gradiant Noise}

$$\hat{g} = g(\Phi) + (\hat{g} - g(\Phi))$$

\vfill
$\hat{g} - g(\Phi)$ has zero mean.

\begin{eqnarray*}
\Phi_{i+N} & \approx & \Phi_i - \eta N g(\Phi)  - \eta \sum_{j=1}^N (\hat{g}_i - g(\Phi))
\end{eqnarray*}

\vfill
We pick $N$ large enough that $\sum_{j=1}^N (\hat{g}_i - g(\Phi))$ is approximately Gaussian.

\slide{Gradiant Noise}

\begin{eqnarray*}
\Phi_{i+N} & \approx & \Phi_i - \eta N g(\Phi)  - \eta \sum_{j=1}^N (\hat{g}_i - g(\Phi)) \\
\\
& \approx & \Phi_i - \eta N g(\Phi)  - \eta \sqrt{N} \epsilon,\;\;\; \epsilon \sim {\cal N}(0,\Sigma)
\end{eqnarray*}

\vfill
Now define $\Delta t = N\eta$ or $N = \Delta t/\eta$.

\begin{eqnarray*}
\Phi(t+ \Delta t) & \approx & \Phi(t) - g(\Phi)\Delta t +  \eta\epsilon \sqrt{ \Delta t/\eta},\;\;\; \epsilon \sim {\cal N}(0,\Sigma) \\
\\
& = & \Phi(t) - g(\Phi)\Delta t +  \sqrt{\eta}\epsilon \sqrt{\Delta t},\;\;\; \epsilon \sim {\cal N}(0,\Sigma)
\end{eqnarray*}


\slide{The SGD SDE}

\begin{eqnarray*}
\Phi(t+ \Delta t) & \approx & \Phi(t) - g(\Phi)\Delta t +  \sqrt{\eta}\epsilon \sqrt{ \Delta t},\;\;\; \epsilon \sim {\cal N}(0,\Sigma) \\
\\
\\
& = & \Phi(t) - g(\Phi)\Delta t +  \sqrt{\eta}\sigma(\Phi) \epsilon \sqrt{ \Delta t},\;\;\; \epsilon \sim {\cal N}(0,I) 
\end{eqnarray*}

\vfill
Here the matrix $\sigma(\Phi)$ is the square root of the covariance matrix $\Sigma(\Phi)$.

\slide{The SGD SDE in One Dimension}

\begin{eqnarray*}
\Phi(t + \Delta t) & = & \Phi(t) -g(\Phi)\Delta t + \sqrt{\eta} \sigma(\Phi) \epsilon\sqrt{\Delta t}
\end{eqnarray*}

\vfill
In one dimension, if the the gradient noise $\sigma(\Phi)$ is constant, then the SGD SDE has the same form as Langevin dynamics and we get.

\begin{eqnarray*}
p(x) & = & \frac{1}{Z} \exp\left(\frac{-2{\cal L}(x)}{ \eta \sigma^2}\right)
\end{eqnarray*}

\vfill
This is Gibbs and provides an interpretation of the learning rate as temperature.


\slide{The SGD SDE in Higher Dimension}

\begin{eqnarray*}
\Phi(t + \Delta t) & = & \Phi(t) -g(\Phi)\Delta t + \sqrt{\eta} \sigma(\Phi) \epsilon\sqrt{\Delta t}
\end{eqnarray*}

\vfill
This is almost the general case of an SDE.

\vfill
Here $g(\Phi)$ is the gradient of a scalar function.  This is not true for a general SDE.

\vfill
But the matrix $\sigma(\Phi)$ is arbitrary.

\vfill
Here the learning rate $\eta$ controls the level of noise but we do not in general have a Gibbs distribution.


\slide{The SGD SDE, A Counter Example}

If we have two dimensions $x$ and $y$ where the loss separates as ${\cal L}(x,y) = {\cal L}(x) + {\cal L}(y)$, and the matrix $\sigma(\Phi)$ is constant and diagonal,
each dimension behaves as an independent one dimensional SGD and we get.

\begin{eqnarray*}
p(x,y) & = & \frac{1}{Z} \exp\left(\frac{-2{\cal L}(x)}{ \eta \sigma_x^2} + \frac{-2{\cal L}(y)}{\eta \sigma_y^2}\right)
\end{eqnarray*}

\vfill
This is not Gibbs.

\slide{END}

\slide{Appendix: Diffusion Flow}



{\Large
We consider the one dimensional case where we have a function $x(t) \in \mathbb{R}$.  We consider a very small time step $\Delta t$ and consider only the diffusion flow.

\begin{eqnarray*}
x(t + \Delta t) & = & x(t) + \sigma \epsilon \sqrt{\Delta t},\;\;\;\epsilon \sim {\cal N}(0,1)
\end{eqnarray*}

\vfill
We assume a density $p_x$ of values of $x$ and let $p_\epsilon(\epsilon)$ be the normal distribution ${\cal N}(0,1)$ on $\epsilon$.

\vfill
The quantity of mass transfer in the time interval $\Delta t$ from values above $x$ to values below $x$ is

\begin{eqnarray*}
& & \int_{z = 0}^\infty  p_x(x + z)\;p_\epsilon(\sigma\epsilon\sqrt{\Delta t} \leq -z) dz  \\
\\
& = & \int_{z = 0}^\infty  p_x(x + z)\;p_\epsilon\left(\epsilon \leq \frac{-z}{\sigma\sqrt{\Delta t}}\right) dz  \\
\\
& =  & \int_{z = 0}^\infty p_x(x+z)\;\Phi\left(\frac{-z}{\sigma\sqrt{\Delta t}}\right) dz
\end{eqnarray*}

\vfill
where $\Phi$ is the cummulative function of the Gaussian.
}

\slide{Appendix: Diffusion Flow}
{\Large

The quantity of mass transfer in the time interval $\Delta t$ from values above $x$ to values below $x$ is


\begin{eqnarray*}
&  & \int_{z = 0}^\infty p_x(x+z)\;\Phi\left(\frac{-z}{\sigma\sqrt{\Delta t}}\right) dz
\end{eqnarray*}

By a change of variables $u = z/(\sigma\sqrt{\Delta t})$ we get

\begin{eqnarray*}
&  & \int_{u = 0}^\infty p_x(x+\sigma\sqrt{\Delta t}\;u)\;\Phi(-u) \sigma\sqrt{\Delta t}\;du
\end{eqnarray*}

\vfill
As $\Delta t \rightarrow 0$ we can use the first order Taylor expansion of the density.

\begin{eqnarray*}
&  & \sigma\sqrt{\Delta t} \int_{u = 0}^\infty \left(p_x(x)+\sigma\sqrt{\Delta t}\;u \frac{dp_x(x)}{dx}\right)\;\Phi(-u)\;du
\end{eqnarray*}
}

\slide{Appendix: Diffusion Flow}
{\Large

\begin{eqnarray*}
&  & \sigma\sqrt{\Delta t} \int_{u = 0}^\infty \left(p_x(x)+\sigma\sqrt{\Delta t}\;u \frac{dp_x(x)}{dx}\right)\;\Phi(-u)\;du \\
\\
& = & \sigma\sqrt{\Delta t}\;p_x(x)\left(\int_{u=0}^\infty \Phi(-u)\;du\right) +  \sigma^2\Delta t \frac{dp_x(x)}{dx} \left(\int_{u=0}^\infty u\Phi(-u) du\right)
\end{eqnarray*}

A similar alanysis shows that the mass transfer from lower values to higher values is

\begin{eqnarray*}
& = & \sigma\sqrt{\Delta t}\;p_x(x)\left(\int_{u=0}^\infty \Phi(-u)\;du\right) -  \sigma^2\Delta t \frac{dp_x(x)}{dx} \left(\int_{u=0}^\infty u\Phi(-u) du\right)
\end{eqnarray*}

\vfill
The net mass transfer in the positive $x$ direction is the second minus the first or

\begin{eqnarray*}
& = & - 2\sigma^2\Delta t \frac{dp_x(x)}{dx} \left(\int_{u=0}^\infty u\Phi(-u) du\right)
\end{eqnarray*}
}

\slide{Appendix: Diffusion Flow}
{\Large

The net mass transfer in the positive $x$ direction is

\begin{eqnarray*}
& & - 2\sigma^2\Delta t \frac{dp_x(x)}{dx} \left(\int_{u=0}^\infty u\Phi(-u) du\right)
\end{eqnarray*}

\vfill
Note that the mass transfer is proportional to $\Delta t$.  Dividing by $\Delta t$ gives the flow per unit time.

\vfill
$$\mbox{Diffusion flow}\;\;= - \alpha \sigma^2 \frac{dp_x(x)}{dx}\;\;\;\;\;\alpha = 2\int_{u=0}^\infty u\Phi(-u) du$$

\vfill
$\alpha$ can be calculated using integration by parts.

\begin{eqnarray*}
\alpha & = & 2 \int_{0}^\infty u \Phi(-u)du \\
& = & \int_{0}^\infty \Phi(-u)du^2 \\
& = & u^2 \Phi(-u)|_{0}^{\infty}+\int_{0}^\infty u^2 \phi(-u)du \;\;\mbox{where $\phi$ is the Gaussian density} \\
& = & \int_{0}^\infty u^2 \phi(-u)du\\
& = & \frac{1}{2}
\end{eqnarray*}}

\slide{Appendix: Diffusion Flow}

{\Large
We now have that the diffusion flow is

$$\mbox{Diffusion flow}\;\;= - \frac{1}{2}\; \sigma^2 \frac{dp_x(x)}{dx}$$

\vfill
For dimension larger than 1 we have

\vfill
$$\mbox{Diffusion flow}\;\;= - \frac{1}{2}\;\Sigma \nabla_x p_x$$

\vfill
Where $\Sigma = E\;(\hat{g} - g)(\hat{g} - g)^\top$ is the covariance matrix of the gradient noise.

\vfill
Here we have derived this from first principle but it also follows from the Fokkerâ€“Planck equation (see Wikipedia).
}

\slide{END}

\end{document}
